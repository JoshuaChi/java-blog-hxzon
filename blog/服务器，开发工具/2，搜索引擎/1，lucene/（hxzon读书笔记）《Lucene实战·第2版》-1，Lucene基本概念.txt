（hxzon读书笔记）《Lucene实战·第2版》-1，Lucene基本概念

by hxzon
原书Lucene 3.0版，当前Lucene 4.8.1版。
====
1，p13
搜索质量主要由“查准率（precision）”和“查全率（recall）”来衡量。

====
2，p15
常见的搜索理论模型有如下3种：

-1，纯布尔模型
文档不管是否匹配查询请求，都不会被评分。
在该模型下，匹配文档与评分不相关，也是无序的。
一条查询近获取所有匹配文档集合的一个子集。

-2，向量空间模型（vector space model）
查询语句和文档都是高维空间的向量模型，这里每一个独立的项都是一个维度。
查询语句与文档之间的相关性或相似性，由各自向量之间的距离计算得到。

-3，概率模型（probabilistic model）
在该模型中，采用全概率方法来计算文档和查询语句的匹配概率。

Lucene在实现上，采用“向量空间模型”和“纯布尔模型”。
并能针对具体搜索，让你决定采用哪种模型。

====
3，p17
搜索范围（scaling）
搜索范围有两种界定方式：“净处理内容”和“净查询吞吐量”。
Lucene并没有提供有关搜索范围的处理模块。

=================
4，索引过程中的核心类

-1，IndexWriter，索引书写器
-2，Directory，索引的存放位置
-3，Analyzer，分析器（索引期间，查询期间）
-4，Document，文档（Lucene只处理文本和数字）
-5，Field，文档中的字段（代表文档，或者文档相关的一些元数据）

token，语汇单元

norms，加权基准。p47
（hxzon：在线性代数，函数分析等数学学科中，norm是一种可以在向量空间里对向量赋予长度和大小的函数。）

stop words，停用词，很常用单没有实际意义的词，例如 a ，an ， on 。

filter，过滤器。例如 PorterStemFilter：处理英文文本，词干提取（词干即词的基本形式）。

DocValues（也称为跨列字段）。4.0版本开始。

Facet，分面。

----
文档可能拥有不只一个同名的字段，在这种情况下，
字段的值就按照索引操作顺序添加进去。
在搜索时，所有字段的文本就好像连接在一起，作为一个字段来处理。

----
在 Solr 和 Lucene 中，Analyzer 包括一个 Tokenizer 和一个或多个 TokenFilter。
Tokenizer 负责 生成 Token，后者在多数情况下对应要索引的词。
TokenFilter 从 Tokenizer 接受 Token 并且可以在索引之前修改或删除 Token。 

例如，Solr 的 WhitespaceTokenizer 根据空白断词，而它的 StopFilter 从搜索结果中删除公共词。
其他类型的分析包括词干提取、同义词扩展和大小写折叠。

====
5，搜索过程中的核心类

-1，IndexSearcher，搜索器
-2，Term，项，条目（字段名+字段值）
-3，Query，查询
-4，TermQuery，条目查询，最基本的查询类型。
-5，TopDocs，匹配结果，指向排名中的前n个搜索结果，会记录每个结果的文档id（int）和分数（float）。

Query.setBoost(float)是最有趣的方法，使你告知Lucene某个子查询，
相对于其他子查询，对最后的评分须有更强的贡献。

（第1章结束）

====
第2章：构建索引

6，lucene针对字段，进行3种操作。

-1，是否被索引（即被分析，转换成语汇单元）。

-2，是否存储“项向量”。
项向量可以看做该字段的一个小型反向索引集合，通过该向量能够检索该字段的所有语汇单元。
这个机制有助于实现一些高级功能：
搜索与当前文档相似的文档。

-3，是否存储“原始值（分析前的字段值）”。

只有被“存储”的字段，才会被作为搜索结果展现。

====
7，反向规格化（denormalization）

Lucene要求索引期间，简单化或反向规格化原始数据，即不支持递归，嵌套。

====
8，

====
9，Lucene将输入数据以一种“倒排索引（inverted index）”的数据结构进行存储。

lucene的索引文件目录，有唯一一个“段结构”。

索引段
Lucene索引都包含一个或多个段。
每个段都是一个独立的索引，它包含整个文档索引的一个子集。
每当writer刷新缓冲区添加的文档，或者挂起目录删除操作时，索引文件都会建立一个新段。

在搜索期间，每个段都是单独访问的，但搜索结果是合并后返回的。

每个段都包含多个文件，使用不同的后缀名，来标识该文件对应索引的哪个部分（项向量，存储的字段，倒排索引等）。
如果使用混合文件格式，则会被压缩成一个单一文件，cfs后缀。

还有一个特殊文件，叫“段文件”，文件名为“段_n”。
n是一个整数，每次向索引提交更改时，这个数加一。
该文件指向所有激活的段。
Lucene首先打开该文件，然后才打开它所指向的其它文件。

IndexWriter会周期性的选择一些段，合并到一个新段中，然后删除老的段。
合并段的选取策略由类MergePolicy控制。
具体合并操作由类MergeScheduler控制。

====
10，基本索引操作

addDocument(Document)
addDocument(Document,Analyzer) 指定分析器。

WhitespaceAnalyzer 空白符分析器。

IndexWriter有很多初始化方法。
可以指定专用的IndexDeletionPolicy，或IndexCommit类。
关闭writer，会向目录提交所有变化。
或者commit()，能在不关闭writer的情况下提交变化。

----
deleteDocuments(Term) 删除包含该项的所有文档。
deleteDocuments(Term[])
deleteDocuments(Query)
deleteDocuments(Query[])
deleteAll()

maxDoc()，返回索引中被删除和未被删除的文档总数。
numDoc()，返回索引中未被删除的文档总数。

----
updateDocument(Term,Document)
updateDocument(Term,Document,Analyzer)

Lucene做不到更新文档中的部分字段。
所以只能删除整个旧文档，再添加新文档。

上述更新方法都是内部调用删除文档的方法，所以你自己要确保Term查出的结果唯一。

================
11，字段选项

11.1，索引选项

是否分析（Analyzed）。分解成独立的语汇单元（token），还是作为一个整体。

是否存储norms 。norms记录了索引中的index-time boost信息（会使得搜索时耗费内存）。

Lucene建立倒排索引后，默认情况下回保存所有必要信息以实施向量空间模型。
该模型需要计算文档中出现的term数，以及它们出现的位置。
有时候有些字段只是在布尔搜索时用到，它们不为相关性评分做贡献，
例如权限过滤，日期过滤，
这时可以通过Field.setOmitTermFregAndPositions(true)来跳过对该字段的出现频率和出现位置的索引。

----
11.2，存储选项

是否存储原始值。

----
11.3，项向量选项

with_positions_offsets

====
12，字段的初始化

Field(String name,Reader value,TermVector termVector)
这个字段不能被存储。

Field(String name,TokenStream tokenStream,TermVector termVector)
允许程序对字段值先进行预分析，然后生成TokenStream对象。
这个字段不会被存储，将一直用于分析和索引。

Field(String name,byte[] value,int offset,int length,Store store)
可用来存储二进制字段，例如不参与索引的字段，或没有项向量的字段。

====
13，字段的排序选项

如果字段是数值类型，应使用NumericField 。

====
14，多值字段

例如一篇文章的作者不只一人。

只要文档中出现同名的多值字段，倒排索引和项向量都会在逻辑上将这些字段的语汇单元附加进去，
具体顺序由添加该字段的顺序决定。
可以在分析期间，使用高级选项来控制“附加顺序”的重要细节，
特别是，如何防止对两个不同字段值的匹配搜索。（hxozn：？）

====
15，加权

norms

可以对文档加权，也可以对字段加权。
只有一个方法：setBoost(float)

默认情况下，所有的文档都没有加权值（都是1.0）。

注意，较短的字段有一个隐含的加权，这取决于Lucene的评分算法的具体实现。
索引期间，indexWriter会调用Similarity.lengthNorm方法，
你可以定制自己的实现（setSimilartity）。

Lucene的评分机制包含大量的因子，其中包括加权因子。

----
加权基准（Norms）
p47

在索引期间，字段的所有加权，都被合并为一个单一的浮点数。

文档也有自己的加权值。
Lucene会基于字段的语汇单元数量，自动计算出这些加权值（更短的字段具有更高的加权值）。
这些加权被合并成一个单一的字节值。

在搜索期间，被搜索字段的norms都被加载到内存，并解码还原成浮点数，用于计算相关性评分（relevance score）。

norms在索引期间首次进行计算，后续可以通过indexReader的setNorm方法进行修改（例如用点击表示受欢迎程度）。

norms面临的问题是它在搜索期间的高内存占用。
被搜索文档的每个字段都需要分配一个字节。

注意，如果在索引进行到一半时关闭norms选项，需要对整个索引进行重建。
因为即使只有一个文档字段在索引时包含了norms选项，
在随后的段合并操作中，会扩散，使得所有文档都占用一个字节的norms 。
Lucene不针对norms进行松散存储。

====
16，索引数字，日期，时间

把日期，时间转成数字进行索引。

要索引数字，要选择一个不丢弃数字的分析器。
whitespaceAnalyzer,StandardAnalyzer 不丢弃数字。
SimpleAnalyzer,StopAnalyzer 会丢弃数字。

tire structure（特里结构）p49
lucene会在后台使用一些算法来确认数值是否被成功索引，以便后续能进行“范围搜索”和“数字排序”。
每个数值都使用特里结构进行索引，
它在逻辑上为越来越大的预定义括号数分配了一个单一的数值。
针对每个括号都在索引中分配了一个唯一的项，
因此我们能够很快的在所有文档中检索这个单一的括号。（hxzon：？）
在搜索期间，搜索请求的范围，被转换成等效的括号并集，实现高效的范围搜索和过滤功能。

每个NumericField只能接受单一的数值，但可以向文档中添加多个同名的字段。
最后生成的NumericRangeQuery和numericRangeFilter会将所有值用逻辑or连接起来。
但这种操作对排序的影响却是不确定的。
如果你需要针对一个字段进行排序，那么你必须对只出现一个该字段的各个numericField进行索引。（hxzon：？）

还有一个高级参数precisionStep（精度步长）允许你对连续出现的括号之间的空隙（以bit形式表示）进行控制。
默认的空隙为4bit 。
更小的空隙值会产生更多的特里括号，
这会增大索引尺寸，但也能带来更快的范围搜索效果。

====
17，字段截断（限制输入的尺寸）

MaxFieldLength

MaxFieldLength.Unlimited 不截断。
MaxFieldLength.Limited 只截取字段中的前1000项。

索引期间，可以在任意时刻调整截取限制。

IndexWriter.setInfoStream(System.out) 可以接收很多诊断信息。

（2014.6.22）















