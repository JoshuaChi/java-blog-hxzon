巫云-Clojure程序员的Monad之旅（Part 4）

http://www.cnblogs.com/darkluck99/archive/2012/03/20/2407456.html
翻译自 A Monad Tutorial For Clojure Programmers (Part 4)
http://onclojure.com/2009/04/24/a-monad-tutorial-for-clojure-programmers-part-4/

=====
在本次旅程的最后一节，我将会介绍monad transformer。
我只介绍其中的一种，
然后我会介绍probability monad 以及 如何使用monad transformer来扩展它。

简单来说，monad transformer是一个函数，参数是一个monad，返回值也是一个monad。
返回的monad是通过给传入的monad增加一些功能而产生的变形。
这些增加的功能，是由monad transformer定义的。
许多我前面提到的普通的monad，都有monad transformer 模拟，可以增加功能，使他们成为其它的monad。

考虑一下我们前面讨论过的maybe monad和sequence monad。
maybe monad用来对可能失败的运行返回有效的值nil。
sequence monad 用来对运算返回多值的结果，monad值组成的序列。
把这个两个monad组合成一个新的monad，可以有2种形式：
1）返回多值结果，值nil表示运算失败；
2）或者返回多值结果，或者在失败的情况下返回nil。
形式1）是比较有用的，对于形式2）来说实用价值不大，运算失败只要返回空序列的方式更简便。

那么，我们用什么办法把maybe monad和sequence monad组合起来，
以便用现有的功能来实现我们所需的功能呢？
很遗憾，这是做不到的。
但是，我们可以保持一个monad不变，而把另一个改写成一个monad transformer。
使用这个monad transformer来处理sequence monad（或其他的monad）来获得我们想要的结果。
为了实现需要组合，我们就可以把maybe monad转换成一个 monad transformer，然后把他应用到sequence monad。

首先，看一下这两个monad的定义：


(defmonad maybe-m
   [m-zero   nil
    m-result (fn [v] v)
    m-bind   (fn [mv f]
               (if (nil? mv) nil (f mv)))
    m-plus   (fn [& mvs]
               (first (drop-while nil? mvs)))
    ])
 
(defmonad sequence-m
   [m-result (fn [v]
               (list v))
    m-bind   (fn [mv f]
               (apply concat (map f mv)))
    m-zero   (list)
    m-plus   (fn [& mvs]
               (apply concat mvs))
    ])

然后看一下maybe monad transformer的定义：


(defn maybe-t
  [m]
  (monad [m-result (with-monad m m-result)
          m-bind   (with-monad m
                     (fn [mv f]
                       (m-bind mv
                               (fn [x]
                                 (if (nil? x)
                                   (m-result nil)
                                   (f x))))))
          m-zero   (with-monad m m-zero)
          m-plus   (with-monad m m-plus)
          ]))

在clojure.contrib.monads库中真正的定义，比这个要复杂一些，我稍后会解释，
但我们现在的版本，已经够不错了。
组合后的monad这样定义：

(def maybe-in-sequence-m (maybe-t sequence-m))
 
直接调用这个函数，返回一个monad。
让我们看看m-result的作用，maybe-m的m-result函数是identity，
因此sequence-m的m-result函数，就是我们需要的m-result。
事实正是如此，（with-monad  m m-result）返回m的m-result函数。
同样的结构，我们在m-zero和m-plus中也看到了，
这说明，我们要修改的仅仅是m-bind部分。

组合后的m-bind抵用了sequence-m的m-bind函数，
但是修改了参数，使用一个函数来表示其余的运算。
在调用之前，首先检查参数是否为nil。
如果为nil，就调用原先的函数，
这样组合后的monad同基础monad一样，不需要计算，返回nil。
尽管如此，我们不能只是返回nil，我们必须返回一个有效的monad值
（在我们的例子中是返回序列中的nil元素）。
因此，我们把nil传回给基础monad的m-result，由它来把nil包装到最终所需的结果中。

让我们看看实际的情况：

(domonad maybe-in-sequence-m
  [x [1 2 nil 4]
   y [10 nil 30 40]]
  (+ x y))

输出结果是：

(11 nil 31 41 12 nil 32 42 nil 14 nil 34 44)

正如我们期望的，所有的非nil值计算后都正常，
但是第一眼看到这个结果我们有些惊讶，为什么是4个nil而不是8个呢
（每个输入序列中的nil和分别另外一个序列的4个元素相加）？

为了理解这个原因，我们来看一下maybe-t中的m-bind函数，
在顶层，使用[1 2 nil 4]作为monad值来运算。
他把这个结构传给sequence-m的的m-bind函数，
这个匿名函数总共调用了maybe-t的m-bind函数4次（每个元素1次）。
对于其中3个非nil的值只是普通的+运算；
对于nil值，结果直接返回nil，不进行运算。
这样，第一个输入vector中的nil元素，在结果中产生了一个nil值，剩余的运算部分只执行了3次。
这3次运算，每次又产生了3个有效的值和1个nil，
这样从第二个输入vector，我们就得到了3 x 3 = 9个有效值，
和3 x 1 = 3个nil，
加上从第一个输入vector得到的一个ni，一共是9个值和4个nil。

用什么办法可以获得全部的 4 x 4共计16个运算值吗？
当然，但是不能使用maybe-t。你只能分别使用maybe-m和sequence-m来计算：


(with-monad maybe-m
   (def maybe-+ (m-lift 2 +)))
  
(domonad sequence-m
   [x [1 2 nil 4]
    y [10 nil 30 40]]
   (maybe-+ x y))

如果你使用maybe-t，你总是会被短路逻辑影响：
一旦有一个nil，就返回nil，并不继续进行剩余的计算。
大多数情况下，这符合我们的需要。

maybe-t和sequence-m的组合并没有多少实用价值，
因为一个更简单有效的方法是在计算前把非有效的参数从输入序列中移出。
但是这个例子很简单，又能很好的解释原理。
现在我们准备挑战一个更有实际意义的例子：
使用maybe-t和probability distribution monad。

probability distribution monad用来处理有限概率分布，
比如，在一个有限元素组成的集合里有非0值的概率。
这个概率用一个map来表示，分别是值和它们出现的记录。
有限分布相关的函数和monad在clojure.contrib.probabilities.finite-distributions库中。

巫云@： 
由于目前clojure.contrib.probabilities.finite-distributions的代码跟clojure 1.3.0以上版本不兼容，
这里我是使用clojure 1.2.0进行测试的。
使用leiningen的同学可以参考我的文章《64位window7下配置Clojure+Emacs开发环境》进行配置。
关于出现这个问题的原因可参照这篇文章。

有限分布的一个简单例子：

(use 'clojure.contrib.probabilities.finite-distributions)

(def die (uniform #{1 2 3 4 5 6}))

(prob odd? die)
 
输出1/2，这是扔骰子点数出现偶数的概率。
die的值，是扔骰子使出现的点数及其概率的分布情况：

{6 1/6, 5 1/6, 4 1/6, 3 1/6, 2 1/6, 1 1/6}

假设我们扔2次骰子，然后观察两次点数的和。
他们的概率分布是怎样的呢？

(domonad dist-m
  [d1 die
   d2 die]
  (+ d1 d2))

结果是：

{2 1/36, 3 1/18, 4 1/12, 5 1/9, 6 5/36, 7 1/6, 8 5/36, 9 1/9, 10 1/12, 11 1/18, 12 1/36}

我们来看一下domonad块的内容：
第一次的分布die绑定到d1，第二次的分布die绑定到d2，然后计算d1+d2的分布。
这个例子很简单，概括来说，
每次分布情况取决于上一次分布情况，这样就创建了变量的联合分布。
这个方法被称为“原始取样”。

dist-m这个monad使用组合概率的基本原则：
如果事件A发生的概率是p，事件B发生的概率是q，并且二者是相互独立的（至少互不影响），
那么A和B同事发生的概率为p x q。

看dist-m的定义：

(defmonad dist-m
   [m-result (fn [v] {v 1})
    m-bind   (fn [mv f]
               (letfn [(add-prob [dist [x p]]
                         (assoc dist x (+ (get dist x 0) p)))]
                 (reduce add-prob {}
                        (for [[x p] mv  [y q] (f x)]
                          [y (* q p)]))))
    ])

像往常一样，m-bind中发生了有趣的事情。
第一个参数mv是一个map，里面存放概率分布情况；
第二个参数f是一个函数，代表剩余的运算，
对于每个for里面的概率值，调用这个函数。
for表达式同时遍历输入分布里的概率和（f x）返回的概率分布里的概率，
通过乘法操作计算联合概率，并把结果输出到输出分布。
通过对辅助函数add-prob上使用reduce来完成运算。
add-prob的检查当前的map的值是否存在，
如果存在，更新概率为add后的新值。这是必须的，
因为在（f x）的取样过程中，同一个值如果对应不同的x，则可能被包含多次。
（巫云@：比如 1+ 2 = 3， 2 + 1 = 3，3可能出现多次）

再看一个更有趣的例子，
著名的Monty Hall问题。
在一个电视现场游戏中，玩家面对3扇门，其中只有一个门后面有奖品。
如果玩家选择了正确的门就能得到奖品。
说到这里，这个问题就可以简化为，获奖概率是1/3。

（巫云@：这个游戏也是来自一款电视节目，貌似跟砸蛋很像啊~）

但是这里有个小插曲，在玩家做出选择之后，主持人打开剩余2扇门中的1扇，这扇门后面是没有奖品的。
然后主持人问玩家是否变更原先的选择。这真是个不错策略，是吧。

为了更好的定义这个问题，我们假设主持人是知道奖品位置的，
因此他不会开打有奖品的门。然后我们开始编程：

(def doors #{:A :B :C})
  
(domonad dist-m
   [prize  (uniform doors)
    choice (uniform doors)]
   (if (= choice prize) :win :loose))

一步步的看，
首先，我们从A，B，C三个门中选择一个作为作为放奖品的门，这代表玩家开始游戏前的前奏。
然后玩家开始选择。
最后我们公布结果，输出:win或者:loose。
很明显概率情况毫无异议，{:win 1/3, :loose 2/3}。

这覆盖了玩家不听取主持人建议的情况，
如果他接受了主持人的建议，情况变得更加复杂：

(domonad dist-m
  [prize  (uniform doors)
   choice (uniform doors)
   opened (uniform (disj doors prize choice))
   choice (uniform (disj doors opened choice))]
  (if (= choice prize) :win :loose))

第3步变得最为有趣：
主持人打开一扇未被选择的，并且没有奖品的门。
我们的模型用移除奖品门和被选择门来体现这一步骤，
从结果集合中我们可以看到，结果中包含1个或2个元素，取决于被选择门和奖品门。
然后玩家改变他的选择，转而选择留下的那个门。
在标准的3个门游戏中，这个可选集合里只有1个门，
但是上面的代码适合更多门的情况。大家可以自己尝试一下。

执行的结果是{:loose 1/3, :win 2/3}，说明改变自己的选择是一个更佳的策略。

回到maybe-t，在有限分布库中定义了一个monad：

(def cond-dist-m (maybe-t dist-m))
 
这使nil成为一个特殊值，表达那些我们不想考虑的可能情况。
使用maybe-t和dist-m，你能猜到在分布联合时nil是如何传递的：
对于任何的nil值，任何对它有潜在依赖的分布都不被计算，
并且nil值的概率被整个传递给输出结果中ni的概括。
但是nil是如何进入到分布中的呢？
并且这样做有什么好处呢？l

考虑最后这个问题，分布中引入nil的作用，是为了消除特定的值。
一旦得到最终的分布情况，nil值会被移除，
并且剩余的分布不会产生异常，他们的概率之和为1。
移除nil以及消除异常的操作通过函数normalize-cond来完成。
cond-dist-m是一个计算条件概率的经典方法，
并常常用于辅助贝叶斯推理（各种数据分析中使用的重要技术）。

第一个练习，我们根据输入的分布和断言，来计算一个简单的条件概率。
输出的分布只包含符合断言的值，但是概率分布的结果会被正常化：

(defn cond-prob [pred dist]
   (normalize-cond (domonad cond-dist-m
                     [v dist
                      :when (pred v)]
                     v)))

关键的代码是:when语句，正如我在第一和第二节提到的那样。
domonad表达式展开为：

(m-bind dist
         (fn [v]
           (if (pred v)
             (m-result v)
              m-zero)))
 
如果你前面仔细留心，你可能要抱怨：
使用dist-m和maybe-t，cond-dist-m应该不需要m-zero。
但是，我前面说过，这里使用的maybe-t是一个简化版，
真正的maybe-t要检查参数monad是否有m-zero，
如果没有，用自己的m-zero函数(with-monad m (m-result nil))来代替它。
因此cond-dist-m的m-zero是{nil 1} ，这个分布的唯一值就是nil 。

domonad在这里起的唯一作用就是保持所有符合断言的值的概率保持不变。
调用normalize-cond去除nil，并用有效值的概率重建分布结果：

(cond-prob odd? die)
-> {5 1/3, 3 1/3, 1 1/3}

cond-dist-m在解决贝叶斯推理时变得太有趣了。
贝叶斯推理是一项描绘不完全观察推论的技术。
有广阔的应用领域，从垃圾邮件过滤到天气预报。
关于这个推论的数学基础可以查看wiki。

这里我们要讨论一个很简单的推理问题和它在Clojure中的解决方案。
假设你有2个骰子，第一个6个面，第二个8个面，第三个12个面。
一个人拿起一个骰子，投几次，然后告诉点数，但并不告诉我们用的是哪个骰子。
根据现象，我们来计算每个骰子被选中的可能性。
我们定义一个函数，返回拥有n个面的骰子的点数分布概率：

(defn die-n [n] (uniform (range 1 (inc n))))
 
接下来，我们参考贝叶斯推理的核心知识。
中心要素是考虑使用过的骰子扔出的点数的分布情况。
我们需要每个骰子的概率分布：

(def dice {:six     (die-n 6)
            :eight   (die-n 8 )
            :twelve  (die-n 12)})
 
另外一个中心要素是体现选择骰子选取优先顺序的分布概率。
我们对此没有定义，每个骰子被使用的概率是完全一样的：

(def prior (uniform (keys dice)))
 
现在我们开始写推理函数。
参数是选取的优先分布和观察的点数，返回一个综合了优先顺序和点数信息的归纳分布。

(defn add-observation [prior observation]
   (normalize-cond
     (domonad cond-dist-m
       [die    prior
        number (get dice die)
        :when  (= number observation)]
       die)))

看一下domonad，第一步根据优先级选取骰子；
第二步，扔骰子获得一个点数；
第三步，去除不在观察范围内的点数；
最后返回die的分布。

建议把这个函数和贝叶斯定理进行比较。
贝叶斯定理P(H|E) = P(E|H) P(H) / P(E) ， 其中H表示假设（hypothesis，假设选择了X骰子），
E表示事实（evidence，观察的现象是扔出的点数N）。
P(H)是优先序列。
这个公式必须在确定的值E上使用。

domonad第一行实现了P(H)，第二行实现了P(E|H)。
这两行合在一起就是一个我们前面提到的P(E, H)原始取样。
:when代表了观察现象；
我们希望把贝叶斯定理使用到确定的值E。
一旦E确定了，P(E)就是一个数字。
最后normalize-cond对它进行正常化。

让我们看一下在观察到1的情况下是什么结果：

(add-observation prior 1)
-> {:twelve 2/9, :eight 1/3, :six 4/9}

我们看到概率最高的的:six，其次是:eight，最小的是:twelve。
这是因为，1在3个骰子上都存在，
但是它在6个面的骰子上，出现概率是1/6，
在8面和12面上的概率自然是1/8和1/12。
这次观察的结果倾向于面较少的骰子。

如果我们观察3次，我们可以重复调用add-observation函数：

(-> prior 
    (add-observation 1)
    (add-observation 3)
    (add-observation 7))
-> {:twelve 8/35, :eight 27/35}

现在我们看到:six消失了，因为观察到了7；
接下来，:eight比:twelve得到了更多的青睐，
也进一步验证了面较少的骰子，被选中的可能性较大。

这个定理在解决垃圾邮件过滤问题时的情况类似。
在这个情况下，3个骰子被替换成选项:spam和:no-spam。
对每一个选项，我们通过分析邮件正文得到一个词的分布概率。
add-observation除了变量名函数完全相同。
当我们对每一个想要分析的词进行评估的时候，
可以根据数据库存储的对它的:spam和:no-spam的选择次数，算出一个优先顺序分布。

在介绍monad transformers内容的最后，
我来解释一下maybe-t的m-zero问题。
正如你知道的，mabye-m有一个m-zero函数(nil)和一个m-plus的定义，
它们在被maybe-t使用时是可以被移除的。
这就是为什么看到cond-dist-m要那么做的原因。
尽管如此，就像sequence-m一样，基础的monad可以拥有自己的m-zero和m-plus。
那么组合的monad应该定义那些内容呢？
只有maybe-t的作者才能做出决定，
所以maybe-t在这里有一个可选参数（看相关文档）。
我们唯一可以明确的一点是，
当一个基础的monad不包含m-zero和m-plus的时候，maybe-t肯定不会对它造成影响。

巫云@： 
最后，偶再扼要总结一下这个系列讲到的monad的基本知识点：

1. monad是一种函数式编程常用的方法，把依赖前一步运算结果的多步运算组成一个运算。

2. monad定义中包括m-result，m-bind，m-zero和m-plus。
其中m-result和m-bind是必须定义的：
m-result把每一个运算的结果包装后传递给m-bind剩余的执行步骤；
m-bind根据绑定表达式把多步操作组成一个链。

3. domonad宏可以简化代码，它展开后成为一个(with monad ...)的block，
包含m-bind，m-result等组成的运算结构。

4. 通过monad transformer可以把一个monad进行功能修改，变成另一个monad。

5. 一些常用的monad：identity-m, maybe-m, sequence-m, state-m, dist-m等。

好了，这个系列的4篇文章翻译完了。
虽然内容包含许多函数编程和数学知识，不是很容易理解，
但是如果大家仔细读过，也一定会有所收获的，感谢大家的支持！

========
========
April 24, 2009

A monad tutorial for Clojure programmers (part 4)

Filed under: Libraries — Konrad Hinsen @ 4:10 pm
In this fourth and last part of my monad tutorial, I will write about monad transformers. I will deal with only one of them, but it’s a start. I will also cover the probability monad, and how it can be extended using a monad transformer.

Basically, a monad transformer is a function that takes a monad argument and returns another monad. The returned monad is a variant of the one passed in to which some functionality has been added. The monad transformer defines that added functionality. Many of the common monads that I have presented before have monad transformer analogs that add the monad’s functionality to another monad. This makes monads modular by permitting client code to assemble monad building blocks into a customized monad that is just right for the task at hand.

Consider two monads that I have discussed before: the maybe monad and the sequence monad. The maybe monad is for computations that can fail to produce a valid value, and return nil in that case. The sequence monad is for computations that return multiple results, in the form of monadic values that are sequences. A monad combining the two can take two forms: 1) computations yielding multiple results, any of which could be nil indicating failure 2) computations yielding either a sequence of results or nil in the case of failure. The more interesting combination is 1), because 2) is of little practical use: failure can be represented more easily and with no additional effort by returning an empty result sequence.

So how can we create a monad that puts the maybe monad functionality inside sequence monad values? Is there a way we can reuse the existing implementations of the maybe monad and the sequence monad? It turns out that this is not possible, but we can keep one and rewrite the other one as a monad transformer, which we can then apply to the sequence monad (or in fact some other monad) to get the desired result. To get the combination we want, we need to turn the maybe monad into a transformer and apply it to the sequence monad.

First, as a reminder, the definitions of the maybe and the sequence monads:

(defmonad maybe-m
   [m-zero   nil
    m-result (fn [v] v)
    m-bind   (fn [mv f]
               (if (nil? mv) nil (f mv)))
    m-plus   (fn [& mvs]
               (first (drop-while nil? mvs)))
    ])

(defmonad sequence-m
   [m-result (fn [v]
               (list v))
    m-bind   (fn [mv f]
               (apply concat (map f mv)))
    m-zero   (list)
    m-plus   (fn [& mvs]
               (apply concat mvs))
    ])
And now the definition of the maybe monad transformer:

(defn maybe-t
  [m]
  (monad [m-result (with-monad m m-result)
          m-bind   (with-monad m
                     (fn [mv f]
                       (m-bind mv
                               (fn [x]
                                 (if (nil? x)
                                   (m-result nil)
                                   (f x))))))
          m-zero   (with-monad m m-zero)
          m-plus   (with-monad m m-plus)
          ]))
The real definition in clojure.algo.monads is a bit more complicated, and I will explain the differences later, but for now this basic version is good enough. The combined monad is constructed by

(def maybe-in-sequence-m (maybe-t sequence-m))
which is a straightforward function call, the result of which is a monad. Let’s first look at what m-result does. The m-result of maybe-m is the identity function, so we’d expect that our combined monad m-result is just the one from sequence-m. This is indeed the case, as (with-monad m m-result) returns the m-result function from monad m. We see the same construct for m-zero and m-plus, meaning that all we need to understand is m-bind.

The combined m-bind calls the m-bind of the base monad (sequence-m in our case), but it modifies the function argument, i.e. the function that represents the rest of the computation. Before calling it, it first checks if its argument would
be nil. If it isn’t, the original function is called, meaning that the combined monad behaves just like the base monad as long as no computation ever returns nil. If there is a nil value, the maybe monad says that no further computation should take place and that the final result should immediately be nil. However, we can’t just return nil, as we must return a valid monadic value in the combined monad (in our example, a sequence of possibly-nil values). So we feed nil into the base monad’s m-result, which takes care of wrapping up nil in the required data structure.

Let’s see it in action:

(domonad maybe-in-sequence-m
  [x [1 2 nil 4]
   y [10 nil 30 40]]
  (+ x y))
The output is:

(11 nil 31 41 12 nil 32 42 nil 14 nil 34 44)
As expected, there are all the combinations of non-nil values in both input sequences. However, it is surprising at first sight that there are four nil entries. Shouldn’t there be eight, resulting from the combinations of a nil in one sequence with the four values in the other sequence?

To understand why there are four nils, let’s look again at how the m-bind definition in maybe-t handles them. At the top level, it will be called with the vector [1 2 nil 4] as the monadic value. It hands this to the m-bind of sequence-m, which calls the
anonymous function in maybe-t‘s m-bind four times, once for each element of the vector. For the three non-nil values, no special treatment is added. For the one nil value, the net result of the computation is nil and the rest of the computation is never called. The nil in the first input vector thus accounts for one nil in the result, and the rest of the computation is called three times. Each of these three rounds produces then three valid results and one nil. We thus have 3×3 valid results, 3×1 nil from the second vector, plus the one nil from the first vector. That makes nine valid results and four nils.

Is there a way to get all sixteen combinations, with all the possible nil results in the result? Yes, but not using the maybe-t transformer. You have to use the maybe and the sequence monads separately, for example like this:

(with-monad maybe-m
  (def maybe-+ (m-lift 2 +)))

(domonad sequence-m
  [x [1 2 nil 4]
   y [10 nil 30 40]]
  (maybe-+ x y))
When you use maybe-t, you always get the shortcutting behaviour seen above: as soon as there is a nil, the total result is nil and the rest of the computation is never executed. In most situations, that’s what you want.

The combination of maybe-t and sequence-m is not so useful in practice because a much easier (and more efficient) way to handle invalid results is to remove them from the sequences before any further processing happens. But the example is simple and thus fine for explaining the basics. You are now ready for a more realistic example: the use of maybe-t with the
probability distribution monad.

The probability distribution monad is made for working with finite probability distributions, i.e. probability distributions in which a finite set of values has a non-zero probability. Such a distribution is represented by a map from the values to their probabilities. The monad and various useful functions for working with finite distributions is defined in the
library clojure.contrib.probabilities.finite-distributions (NOTE: this module has not yet been migrated to the new Clojure contrib library set.).

A simple example of a finite distribution:

(use 'clojure.contrib.probabilities.finite-distributions)
(def die (uniform #{1 2 3 4 5 6}))
(prob odd? die)
This prints 1/2, the probability that throwing a single die yields an odd number. The value of die is the probability distribution of the outcome of throwing a die:

{6 1/6, 5 1/6, 4 1/6, 3 1/6, 2 1/6, 1 1/6}
Suppose we throw the die twice and look at the sum of the two values. What is its probability distribution? That’s where the monad comes in:

(domonad dist-m
  [d1 die
   d2 die]
  (+ d1 d2))
The result is:

{2 1/36, 3 1/18, 4 1/12, 5 1/9, 6 5/36, 7 1/6, 8 5/36, 9 1/9, 10 1/12, 11 1/18, 12 1/36}
You can read the above domonad block as ‘draw a value from the distribution die and call it d1, draw a value from the distribution die and call it d2, then give me the distribution of (+ d1 d2)‘. This is a very simple example; in general, each distribution can depend on the values drawn from the preceding ones, thus creating the joint distribution of several variables. This approach is known as ‘ancestral sampling’.

The monad dist-m applies the basic rule of combining probabilities: if event A has probability p and event B has probability q, and if the events are independent (or at least uncorrelated), then the probability of the combined event (A and B) is p*q. Here is the definition of dist-m:

(defmonad dist-m
  [m-result (fn [v] {v 1})
   m-bind   (fn [mv f]
          (letfn [(add-prob [dist [x p]]
                 (assoc dist x (+ (get dist x 0) p)))]
            (reduce add-prob {}
                (for [[x p] mv  [y q] (f x)]
              [y (* q p)]))))
   ])
As usually, the interesting stuff happens in m-bind. Its first argument, mv, is a map representing a probability distribution. Its second argument, f, is a function representing the rest of the calculation. It is called for each possible value in the probability distribution in the for form. This for form iterates over both the possible values of the input distribution and the possible values of the distribution returned by (f x), combining the probabilities by multiplication and putting them into the output distribution. This is done by reducing over the helper function add-prob, which checks if the value is already present in the map, and if so, adds the probability to the previously obtained one. This is necessary because the samples from the (f x) distribution can contain the same value more than once if they were obtained for different x.

For a more interesting example, let’s consider the famous Monty Hall problem. In a game show, the player faces three doors. A prize is waiting for him behind one of them, but there is nothing behind the two other ones. If he picks the right door, he gets the prize. Up to there, the problem is simple: the probability of winning is 1/3.

But there is a twist. After the player makes his choice, the game host open one of the two other doors, which shows an empty space. He then asks the player if he wants to change his mind and choose the last remaining door instead of his initial choice. Is this a good strategy?

To make this a well-defined problem, we have to assume that the game host knows where the prize is and that he would not open the corresponding door. Then we can start coding:

(def doors #{:A :B :C})

(domonad dist-m
  [prize  (uniform doors)
   choice (uniform doors)]
  (if (= choice prize) :win :loose))
Let’s go through this step by step. First, we choose the prize door by drawing from a uniform distribution over the three doors :A, :B, and :C. That represents what happens before the player comes in. Then the player’s initial choice is made, drawing from the same distribution. Finally, we ask for the distribution of the outcome of the game, code>:win or :loose. The answer is, unsurprisingly, {:win 1/3, :loose 2/3}.

This covers the case in which the player does not accept the host’s proposition to change his mind. If he does, the game becomes more complicated:

(domonad dist-m
  [prize  (uniform doors)
   choice (uniform doors)
   opened (uniform (disj doors prize choice))
   choice (uniform (disj doors opened choice))]
  (if (= choice prize) :win :loose))
The third step is the most interesting one: the game host opens a door which is neither the prize door nor the initial choice of the player. We model this by removing both prize and choice from the set of doors, and draw uniformly from the resulting set, which can have one or two elements depending on prize and choice. The player then changes his mind and chooses from the set of doors other than the open one and his initial choice. With the standard three-door game, that set has exactly one element, but the code above also works for a larger number of doors – try it out yourself!

Evaluating this piece of code yields {:loose 1/3, :win 2/3}, indicating that the change-your-mind strategy is indeed the better one.

Back to the maybe-t transformer. The finite-distribution library defines a second monad by

(def cond-dist-m (maybe-t dist-m))
This makes nil a special value in distributions, which is used to represent events that we don’t want to consider as possible ones. With the definitions of maybe-t and dist-m, you can guess how nil values are propagated when distributions are combined: for any nil value, the distributions that potentially depend on it are never evaluated, and the nil value’s probability is transferred entirely to the probability of nil in the output distribution. But how does nil ever get into a distribution? And, most of all, what is that good for?

Let’s start with the last question. The goal of this nil-containing distributions is to eliminate certain values. Once the final distribution is obtained, the nil value is removed, and the remaining distribution is normalized to make the sum of the probabilities of the remaining values equal to one. This nil-removal and normalization is performed by the utility function normalize-cond. The cond-dist-m monad is thus a sophisticated way to compute conditional probabilities, and in particular to facilitate Bayesian inference, which is an important technique in all kinds of data analysis.

As a first exercice, let’s calculate a simple conditional probability from an input distribution and a predicate. The output distribution should contain only the values satisfying the predicate, but be normalized to one:

(defn cond-prob [pred dist]
  (normalize-cond (domonad cond-dist-m
                    [v dist
                     :when (pred v)]
                    v))))
The important line is the one with the :when condition. As I have explained in parts 1 and 2, the domonad form becomes

(m-bind dist
        (fn [v]
          (if (pred v)
            (m-result v)
             m-zero)))
If you have been following carefully, you should complain now: with the definitions of dist-m and maybe-t I have given above, cond-dist-m should not have any m-zero! But as I said earlier, the maybe-t shown here is a simplified version. The real one checks if the base monad has m-zero, and if it hasn’t, it substitutes its own, which is (with-monad m (m-result nil)). Therefore the m-zero of cond-dist-m is {nil 1}, the distribution whose only value is nil.

The net effect of the domonad form in this example is thus to keep all values that satisfy the predicate with their initial probabilities, but to transfer the probability of all values to nil. The call to normalize-cond then takes out the nil and re-distributes its probability to the other values. Example:

(cond-prob odd? die)
-> {5 1/3, 3 1/3, 1 1/3}
The cond-dist-m monad really becomes interesting for Bayesian inference problems. Bayesian inference is technique for drawing conclusions from incomplete observations. It has a wide range of applications, from spam filters to weather forecasts. For an introduction to the technique and its mathematical basis, you can start with the Wikipedia article.

Here I will discuss a very simple inference problem and its solution in Clojure. Suppose someone has three dice, one with six faces, one with eight, and one with twelve. This person picks one die, throws it a few times, and gives us the numbers, but doesn’t tell us which die it was. Given these observations, we would like to infer the probabilities for each of the three dice to have been picked. We start by defining a function that returns the distribution of a die with n faces:

(defn die-n [n] (uniform (range 1 (inc n))))
Next, we come to the core of Bayesian inference. One central ingredient is the probability for throwing a given number under the assumption that die X was used. We thus need the probability distributions for each of our three dice:

(def dice {:six     (die-n 6)
           :eight   (die-n 8 )
           :twelve  (die-n 12)})
The other central ingredient is a distribution representing our ‘prior knowledge’ about the chosen die. We actually know nothing at all, so each die has the same weight in this distribution:

(def prior (uniform (keys dice)))
Now we can write the inference function. It takes as input the prior-knowledge distribution and a number that was obtained from the die. It returns the a posteriori distribution that combines the prior information with the information from the observation.

(defn add-observation [prior observation]
  (normalize-cond
    (domonad cond-dist-m
      [die    prior
       number (get dice die)
       :when  (= number observation)]
      die)))
Let’s look at the domonad form. The first step picks one die according to the prior knowledge. The second line “throws” that die, obtaining a number. The third line eliminates the numbers that don’t match the observation. And then we ask for the distribution of the die.

It is instructive to compare this function with the mathematical formula for Bayes’ theorem, which is the basis of Bayesian inference. Bayes’ theorem is P(H|E) = P(E|H) P(H) / P(E), where H stands for the hypothesis (“the die chosen was X”) and E stands for the evidence (“the number thrown was N”). P(H) is the prior knowledge. The formula must be evaluated for a fixed value of E, which is the observation.

The first line of our domonad form implements P(H), the second line implements P(E|H). These two lines together thus sample P(E, H) using ancestral sampling, as we have seen before. The :when line represents the observation; we wish to apply Bayes’ theorem for a fixed value of E. Once E has been fixed, P(E) is just a number, required for normalization. This is handled by normalize-cond in our code.

Let’s see what happens when we add a single observation:

(add-observation prior 1)
-> {:twelve 2/9, :eight 1/3, :six 4/9}
We see that the highest probability is given to :six, then :eight, and finally :twelve. This happens because 1 is a possible value for all dice, but it is more probable as a result of throwing a six-faced die (1/6) than as a result of throwing an eight-faced die (1/8) or a twelve-faced die (1/12). The observation thus favours a die with a small number of faces.

If we have three observations, we can call add-observation repeatedly:

(-> prior (add-observation 1)
          (add-observation 3)
          (add-observation 7))
-> {:twelve 8/35, :eight 27/35}
Now we see that the candidate :six has disappeared. In fact, the observed value of 7 rules it out completely. Moreover, the observed numbers strongly favour :eight over :twelve, which is again due to the preference for the smallest possible die in the game.

This inference problem is very similar to how a spam filter works. In that case, the three dice are replaced by the choices :spam or :no-spam. For each of them, we have a distribution of words, obtained by analyzing large quantities of e-mail messages. The function add-observation is strictly the same, we’d just pick different variable names. And then we’d call it for each word in the message we wish to evaluate, starting from a prior distribution defined by the total number of :spam and :no-spam messages in our database.

To end this introduction to monad transformers, I will explain the m-zero problem in maybe-t. As you know, the maybe monad has an m-zero definition (nil) and an m-plus definition, and those two can be carried over into a monad created by applying maybe-t to some base monad. This is what we have seen in the case of cond-dist-m. However, the base monad might have its own m-zero and m-plus, as we have seen in the case of sequence-m. Which set of definitions should the combined monad have? Only the user of maybe-t can make that decision, so maybe-t has an optional parameter for this (see its documentation for the details). The only clear case is a base monad without m-zero and m-plus; in that case, nothing is lost if maybe-t imposes its own.

