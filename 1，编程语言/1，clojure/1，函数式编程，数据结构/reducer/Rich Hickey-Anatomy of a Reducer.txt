Rich Hickey-Anatomy of a Reducer
Posted by Rich Hickey on May 15, 2012
http://clojure.com/blog/2012/05/15/anatomy-of-reducer.html

注释by hxzon
========
Last time, I blogged about Clojure's new reducers library. 
This time I'd like to look at the details of what constitutes a reducer, 
as well as some background about the library.

====
What's a Reducing Function?
什么是可规约的函数

The reducers library is built around transforming reducing functions. 
A reducing function is simply a binary function, 
akin to the one you might pass to reduce. 
可规约函数是一个二元函数，输入为上次累积值，和当前迭代值。
While the two arguments might be treated symmetrically by the function, 
there is an implied semantic that distinguishes the arguments: 
the first argument is a result or accumulator that is being built up by the reduction, 
while the second is some new input value from the source being reduced. 
While reduce works from the 'left', 
that is neither a property nor promise of the reducing function, but one of reduce itself. 
So we'll say simply that a reducing fn has the shape:

  
  (f result input) -> new-result
  
In addition, a reducing fn may be called with no args, 
and should then return an identity value for its operation.
“可规约函数”可能不传入参数，此时需要返回 identity 值。

====
Transforming Reducing Functions
普通函数转换为可规约函数

A function that transforms a reducing fn simply takes one, and returns another one:

  
  (xf reducing-fn) -> reducing-fn
  
Many of the core collection operations can be expressed in terms of such a transformation. 
Imagine if we were to define the cores of map, filter and mapcat in this way:

  
  (defn mapping [f]
    (fn [f1]
      (fn [result input]
        (f1 result (f input)))))
        ;; 输入映射函数，返回一个函数，
        ;; 这个函数的输入是组合函数，输出是一个规约函数，


  (defn filtering [pred]
    (fn [f1]
      (fn [result input]
        (if (pred input)
          (f1 result input)
          result))))
          ;; 输入谓词函数，返回一个函数，
          ;; 这个函数的输入是组合函数，输出是一个函数，
          ;; 这个函数是规约函数，但会根据谓词，忽略某些元素

  (defn mapcatting [f]
    (fn [f1]
      (fn [result input]
        (reduce f1 result (f input)))))
  
There are a few things to note:

The functions consist only of the core logic of their operations
That logic does not include any notion of collection, nor order
不包含集合的信息，不包含集合元素的顺序信息
filtering and kin can 'skip' inputs by simply returning the incoming result
过滤函数通过返回上一次规约结果，即跳过了某些元素
mapcatting and kin can produce more than one result per input by simply operating on result more than once
mapcatting函数对每个输入，可产生多个输出，

Using these directly is somewhat odd, 
because we are operating on the reducing operation rather than the collection:

  
  (reduce + 0 (map inc [1 2 3 4]))
  ;;becomes
  (reduce ((mapping inc) +) 0 [1 2 3 4])
  ;; inc 是映射函数，+ 是组合函数

====
Reducers

We expect map/filter etc to take and return logical collections. 
The premise of the reducers library is that 
the minimum definition of collection is something that is reducible. 
reduce ends up using a protocol (CollReduce) to ask the collection to reduce itself, 
so we can make reducible things by extending that protocol. 
继承该协议，使得可规约。
Thus, given a collection and a reducing function transformer like those above, 
we can make a reducible with a function like this:

  ;; 生成一个可规约结构
  (defn reducer
    ([coll xf]
     (reify
      clojure.core.protocols/CollReduce
      (coll-reduce [ _ f1 init]
        (clojure.core.protocols/coll-reduce coll (xf f1) init)))))
  
Now:

  
  (reduce + 0 (map inc [1 2 3 4]))
  ;; 操作序列
  ;;becomes
  (reduce + 0 (reducer [1 2 3 4] (mapping inc)))
  ;; 操作可规约结构

That's better. 
It feels as if we have transformed the collection itself. 
看起来我们是在转换集合。
Note:

reducer ultimately asks the source collection to reduce itself

reducer will work with any reducing function transformer

Another objective of the library is to support reducer-based code 
with the same shape as our current seq-based code. 
另一个好处是，

Getting there is easy:
hxzon：原来的版本生成序列，现在生成“可规约结构”：
  
  (defn rmap [f coll]
    (reducer coll (mapping f)))

  (defn rfilter [pred coll]
    (reducer coll (filtering pred)))

  (defn rmapcat [f coll]
    (reducer coll (mapcatting f)))

  (reduce + 0 (rmap inc [1 2 3 4]))
  ;=> 14

  (reduce + 0 (rfilter even? [1 2 3 4]))
  ;=> 6

  (reduce + 0 (rmapcat range [1 2 3 4 5]))
  ;=> 20
  
====
From Reducible to (Parallel) Foldable
折叠（自动并行）

While it is an interesting exercise 
to find another fundamental way to define the core collection operations, 
the end result is not much different, just faster, 
certainly something a state-of-the-art compilation 
and type system (had we one) might do for us given sequence code. 

To stop here would be to completely miss the point of the library. 
These operations have different, fundamentally simpler semantics than their sequence-based counterparts.

How does one define parallel mapping/filtering/mapcatting etc? 
We already did! 
如何定义并行化版本的映射，过滤和映射拼接？
As long as the transformation itself doesn't care about order (e.g. as take does), 
then a reducer is as foldable as its source. 
As with reduce, fold bottoms out on a protocol (CollFold), and our reducer can extend that:
折叠基于一个协议 CollFold 。

  ;; 生成一个可折叠结构
  (defn folder
    ([coll xf]
       (reify
        ;;extend CollReduce as before

        CollFold
        (coll-fold [ _ n combinef reducef]
          (coll-fold coll n combinef (xf reducef))))))
  
Note that:

folder has the same requirements as reducer - collection + reducing function transformer

when fold is applied to something that can't fold, it devolves to reduce
（devolves，移交）
当折叠应用于不可折叠的数据时，它移交给 reduce 。

Thus the real definitions of reducers/map et al use folder (while take uses reducer):
真正的定义：返回可折叠结构：

  
  (defn rmap [f coll]
    (folder coll (mapping f)))

  (defn rfilter [pred coll]
    (folder coll (filtering pred)))

  (defn rmapcat [f coll]
    (folder coll (mapcatting f)))
  

Thus a wide variety of collection transformations 
can instead be expressed as reducing function transformations, 
and applied in both sequential and parallel contexts, across a wide variety of data structures.
能应用于串行和并行环境。

The library deals with several other details, such as:
reducer库还解决了其它细节：

the transformers all need a nullary arity that just delegates to the transformed reducing function
无参

the transformers support a ternary arity where 2 inputs are supplied per step, as occurs with reduce-kv and map sources
转换器支持三元参数，多出来的参数是键，当用于关联性数据时。

all of the reducers are curried

These additions are all mechanical, and are handled by macros. 
It is my hope that the above will help illuminate the core logic underlying the library.

====
Background
背景

Much prior work highlights the value of fold as a primary mechanism for collection manipulation, 
superior to iteration, 
although most of that work was done in the context of recursively defined functions on lists or sequences 
- i.e. fold implies foldl/foldr, and the results remain inherently sequential.

The two primary motivators for this library were the Haskell Iteratee library and Guy Steele's ICFP '09 talk.

==
Haskell Iteratees

The Haskell Enumerator/Iteratee library and its antecedents 
are an inspiring effort to disentangle the source of data and the operations that might apply to it, 
and one of the first I think to reify the role of the 'iteratee'. 
An enumerator makes successive calls to the iteratee to supply it items, 
decoupling the iteratee from the data source. 
But the iteratee is still driving in some sense, as it is in charge of signaling Done, 
and, it returns on each step the next iteratee to use, effectively dictating a single thread of control. 
One benefit is that even operations like take can be defined functionally, 
as they can encode their internal state in the 'next' iteratee returned. 
OTOH, and unlike reducers, 
the design wraps the result being built up in a new iteratee each step, with potential allocation overhead.

Being an automaton in a state, an iteratee is like a reified left fold, and thus inherently serial. 
So, while they form quite a nice substrate for the design of, e.g. parsers, 
iteratees are unsuitable for defining things like map/filter etc if one intends to be able to parallelize them.

==
Guy Steele's ICFP '09 talk

Organizing Functional Code for Parallel Execution or, foldl and foldr Considered Slightly Harmful

This talk boils down to - stop programming with streams, lists, generators etc 
if you intend to exploit parallelism, as does the reducers library.

Where reducers diverges from that talk is in the structure of the fork/join parallel computation. 
Rather than map+reduce, reducers uses reduce+combine. 

This reflects 2 considerations:

It is accepted fork/join practice that at some point you stop splitting in half and handle the leaves 'sequentially'
if the best way to do that at the top is reduce, why not at the bottom as well?
map forces a result per input

You can see the awkwardness of the latter in the map/reduce-oriented definition of parallel filter in the talk, 
which must 'listify' items or return empty lists, 
creating a bunch of concatenation busy-work for the reducing step. 
Many other collection algorithms suffer similarly in their map/reduce-oriented implementations, 
having greater internal complexity and wrapping the results in collection representations, 
with corresponding creation of more garbage and reduction busy-work etc 
vs the reducing function transformer versions of same.

It is interesting that the accumulator style is not completely absent from the reducers design, 
in fact it is important to the characteristics just described. 
一件有趣的事是，累加器风格，没有出现在 reducers 的设计中，
实际上，
What has been abandoned are the single initial value and serial execution promises of foldl/r.

Summary
I hope this makes reducers easier to understand, use and define.

Rich
